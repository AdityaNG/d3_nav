{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"d3nav D\u00b3Nav: Data-Driven Driving Agents for Autonomous Vehicles in Unstructured Traffic. This repo is my implementation of D3Nav using CommaAI's video world model, fine tuning it using the D3Nav methodology on NuScenes. Abstract Navigating unstructured traffic autonomously requires handling a plethora of edge cases, traditionally challenging for perception and path-planning modules due to scarce real-world data and simulator limitations. By employing the next-token prediction task, LLMs have demonstrated to have learned a world model. D\u00b3Nav bridges this gap by employing a quantized encoding to transform high-dimensional video data (Fx3x128x256) into compact integer embeddings (Fx128) which are fed into our world model. D\u00b3Nav's world model is trained on the next-video-frame prediction task and simultaneously predicts the desired driving signal. The architecture's compact nature enables real-time operation while adhering to stringent power constraints. D\u00b3Nav's training on diverse datasets featuring unstructured data results in the model's proficient prediction of both future video frames and the driving signal. We make use of automated labeling to generate importance masks accentuating pedestrians and vehicles to aid our encoding system in focusing on objects of interest. These capabilities are an improvement in end-to-end autonomous navigation systems, particularly in the context of unstructured traffic environments. Our contribution includes our driving agent D\u00b3Nav and our embeddings dataset of unstructured traffic. We make our code and dataset\\footnote{Please refer to the supplementary material} public. Video Demos Below is a demo of D\u00b3Nav generating video frames Below is a demo of D\u00b3Nav generating the control signal (desired trajectory) on a subset of our dataset. Note that the point cloud and semantic segmentation are not generated by D\u00b3Nav and are only used to visualize the trajectory in 3D with respect to other objects. Cite Cite our work if you find it useful @article{NG2024D3Nav, title={D\u00b3Nav: Data-Driven Driving Agents for Autonomous Vehicles in Unstructured Traffic}, author={Aditya NG and Gowri Srinivas}, journal={The 35th British Machine Vision Conference (BMVC)}, year={2024}, url={https://bmvc2024.org/} } Install it from PyPI pip install d3nav You can run the video demo as follows python3 -m d3nav --video_path input_video.mp4 Usage Following is an example usage of the planner. Look at d3nav/cli.py for more details. from d3nav import load_d3nav, center_crop, d3nav_transform_img, visualize_frame_img # Load the model model = load_d3nav(args.ckpt) model = model.cuda() model.eval() # Create a buffer of 5 seconds buffer_size = int(5 * fps) buffer_full = int(4.5 * fps) frame_history = deque(maxlen=buffer_size) # Load a video and populate the buffer for index in tqdm(range(frame_count), desc=\"Processing frames\"): ret, frame = cap.read() frame = center_crop(frame, crop_ratio) frame_history.append(frame.copy()) if len(frame_history) >= buffer_full: break # Construct the input of 8 frames at FPS history_tensors = [] step = len(frame_history) // 8 for i in range(0, len(frame_history), step): if len(history_tensors) < 8: # Ensure we only get 8 frames frame = frame_history[i] frame = d3nav_transform_img(frame) frame_t = torch.from_numpy(frame) history_tensors.append(frame_t) # Stack the tensors to create sequence sequence = torch.stack(history_tensors) sequence = sequence.unsqueeze(0).cuda() # Add batch dimension # Get trajectory prediction with torch.no_grad(): trajectory = model(sequence) trajectory = trajectory[0].cpu().numpy() # Remove batch dimension # Process trajectory for visualization traj = trajectory[:, [1, 2, 0]] traj[:, 0] *= -1 trajectory = np.vstack(([0, 0, 0], traj)) # Add origin point img_vis, img_bev = visualize_frame_img( img=frame.copy(), trajectory=trajectory, color=(255, 0, 0), ) You can train on the comma dataset using the following script # Train the VQ-VAE for trajectory encoding python3 -m d3nav.scripts.train_traj # Fine tune the video model for trajectory prediction python3 -m d3nav.scripts.train Model Predictive Control Checkout our Model Predictive Controller for computing steering angle and acceleration. # TODO: implement MPC to show steering # pip install model_predictive_control import numpy as np from model_predictive_control.cost.trajectory2d_steering_penalty import ( Traj2DSteeringPenalty, ) from model_predictive_control.models.bicycle import ( BicycleModel, BicycleModelParams, ) from model_predictive_control.mpc import MPC # Initialize the Bicycle Model params = BicycleModelParams( time_step=time_step, steering_ratio=13.27, wheel_base=2.83972, speed_kp=1.0, speed_ki=0.1, speed_kd=0.05, throttle_min=-1.0, throttle_max=1.0, throttle_gain=5.0, # Max throttle corresponds to 5m/s^2 ) bicycle_model = BicycleModel(params) # Define the cost function cost = Traj2DSteeringPenalty(model=bicycle_model) # Initialize MPC Controller horizon = 20 state_dim = 4 # (x, y, theta, velocity) controls_dim = 2 # (steering_angle, velocity) mpc = MPC( model=bicycle_model, cost=cost, horizon=horizon, state_dim=state_dim, controls_dim=controls_dim, ) # Define initial state (x, y, theta, velocity) start_state = [0.0, 0.0, 0.0, 1.0] # Define desired trajectory: moving in a straight line desired_state_sequence = [[i * 1.0, i * 0.5, 0.0, 1.0] for i in range(horizon)] # Initial control sequence: assuming zero steering and constant speed initial_control_sequence = [[0.0, 1.0] for _ in range(horizon)] # Define control bounds: steering_angle between -0.5 and 0.5 radians, bounds = [[(-np.deg2rad(400), np.deg2rad(400)), (-1.0, 1.0)] for _ in range(horizon)] # Optimize control inputs using MPC optimized_control_sequence = mpc.step( start_state_tuple=start_state, desired_state_sequence=desired_state_sequence, initial_control_sequence=initial_control_sequence, bounds=bounds, max_iters=50, ) Development Read the CONTRIBUTING.md file. Getting Started Docker Environment To build, use: DOCKER_BUILDKIT=1 docker-compose build To run the interactive shell, use: docker-compose run dev Future Video Prediction D\u00b3Nav takes 6 frames as input context and produces the next 6 frames. In the prompt columns, we show the last frame of the input and on the Prediction column, we have an animation of D\u00b3Nav's prediction of what it thinks will happen next. Trajectory Demo We have put together a demo video of D\u00b3Nav operating on a subset of our dataset. In the video, D\u00b3Nav takes the video frames as input and produces the control signal (desired trajectory) as output which is plotted out as a red strip on the 3D and 2D views. We have a parallel system which produces 3D semantic occupancy. The 3D semantic occupancy is not produced by D\u00b3Nav and is only plotted to help visualize the trajectory in 3D with respect to other objects. The semantics are highlighted in both the 2D and 3D views (Vehicles in Blue and Pedestrians in Red). All other objects are colored by a height map on the 3D view. The video is placed at DEMO_3_control_signal_trajectory.mp4 Dataset We have provided a subset of our dataset in the BengaluruDrivingEmbeddings folder for review. We have ensured that there is no personally identifyable information (faces, number plates, etc.) in our dataset. Dataset Structure BengaluruDrivingEmbeddings/ \u251c\u2500\u2500 1658384924059 # Dataset ID \u2502 \u251c\u2500\u2500 embeddings # Folder of embeddings \u2502 \u251c\u2500\u2500 embeddings_features_quantized # Folder of quantized embeddings \u2502 \u251c\u2500\u2500 embeddings_index.npy # Integer indices of the embeddings \u2502 \u251c\u2500\u2500 input_video.mp4 # Raw video \u2502 \u2514\u2500\u2500 reconstructed_video.mp4 # Video Reconstructed by VQ-VAE \u251c\u2500\u2500 calibration # Camera Intrinsics \u2502 \u251c\u2500\u2500 calibrationSession.mat \u2502 \u251c\u2500\u2500 calib.txt \u2502 \u2514\u2500\u2500 calib.yaml \u2514\u2500\u2500 weights # VQ-VAE weights \u251c\u2500\u2500 decoder.onnx \u251c\u2500\u2500 decoder.onnx.dynanic_quant.onnx \u251c\u2500\u2500 decoder.pth \u251c\u2500\u2500 encoder.onnx \u251c\u2500\u2500 encoder.onnx.dynanic_quant.onnx \u251c\u2500\u2500 encoder.pth \u251c\u2500\u2500 quantizer_e_i_ts.npy \u251c\u2500\u2500 quantizer.onnx \u251c\u2500\u2500 quantizer.onnx.dynanic_quant.onnx \u2514\u2500\u2500 quantizer.pth Acknowledgement We would like to thank the authors of the following repositories minGPT PySLAM","title":"d3nav"},{"location":"#d3nav","text":"D\u00b3Nav: Data-Driven Driving Agents for Autonomous Vehicles in Unstructured Traffic. This repo is my implementation of D3Nav using CommaAI's video world model, fine tuning it using the D3Nav methodology on NuScenes. Abstract Navigating unstructured traffic autonomously requires handling a plethora of edge cases, traditionally challenging for perception and path-planning modules due to scarce real-world data and simulator limitations. By employing the next-token prediction task, LLMs have demonstrated to have learned a world model. D\u00b3Nav bridges this gap by employing a quantized encoding to transform high-dimensional video data (Fx3x128x256) into compact integer embeddings (Fx128) which are fed into our world model. D\u00b3Nav's world model is trained on the next-video-frame prediction task and simultaneously predicts the desired driving signal. The architecture's compact nature enables real-time operation while adhering to stringent power constraints. D\u00b3Nav's training on diverse datasets featuring unstructured data results in the model's proficient prediction of both future video frames and the driving signal. We make use of automated labeling to generate importance masks accentuating pedestrians and vehicles to aid our encoding system in focusing on objects of interest. These capabilities are an improvement in end-to-end autonomous navigation systems, particularly in the context of unstructured traffic environments. Our contribution includes our driving agent D\u00b3Nav and our embeddings dataset of unstructured traffic. We make our code and dataset\\footnote{Please refer to the supplementary material} public.","title":"d3nav"},{"location":"#video-demos","text":"Below is a demo of D\u00b3Nav generating video frames Below is a demo of D\u00b3Nav generating the control signal (desired trajectory) on a subset of our dataset. Note that the point cloud and semantic segmentation are not generated by D\u00b3Nav and are only used to visualize the trajectory in 3D with respect to other objects.","title":"Video Demos"},{"location":"#cite","text":"Cite our work if you find it useful @article{NG2024D3Nav, title={D\u00b3Nav: Data-Driven Driving Agents for Autonomous Vehicles in Unstructured Traffic}, author={Aditya NG and Gowri Srinivas}, journal={The 35th British Machine Vision Conference (BMVC)}, year={2024}, url={https://bmvc2024.org/} }","title":"Cite"},{"location":"#install-it-from-pypi","text":"pip install d3nav You can run the video demo as follows python3 -m d3nav --video_path input_video.mp4","title":"Install it from PyPI"},{"location":"#usage","text":"Following is an example usage of the planner. Look at d3nav/cli.py for more details. from d3nav import load_d3nav, center_crop, d3nav_transform_img, visualize_frame_img # Load the model model = load_d3nav(args.ckpt) model = model.cuda() model.eval() # Create a buffer of 5 seconds buffer_size = int(5 * fps) buffer_full = int(4.5 * fps) frame_history = deque(maxlen=buffer_size) # Load a video and populate the buffer for index in tqdm(range(frame_count), desc=\"Processing frames\"): ret, frame = cap.read() frame = center_crop(frame, crop_ratio) frame_history.append(frame.copy()) if len(frame_history) >= buffer_full: break # Construct the input of 8 frames at FPS history_tensors = [] step = len(frame_history) // 8 for i in range(0, len(frame_history), step): if len(history_tensors) < 8: # Ensure we only get 8 frames frame = frame_history[i] frame = d3nav_transform_img(frame) frame_t = torch.from_numpy(frame) history_tensors.append(frame_t) # Stack the tensors to create sequence sequence = torch.stack(history_tensors) sequence = sequence.unsqueeze(0).cuda() # Add batch dimension # Get trajectory prediction with torch.no_grad(): trajectory = model(sequence) trajectory = trajectory[0].cpu().numpy() # Remove batch dimension # Process trajectory for visualization traj = trajectory[:, [1, 2, 0]] traj[:, 0] *= -1 trajectory = np.vstack(([0, 0, 0], traj)) # Add origin point img_vis, img_bev = visualize_frame_img( img=frame.copy(), trajectory=trajectory, color=(255, 0, 0), ) You can train on the comma dataset using the following script # Train the VQ-VAE for trajectory encoding python3 -m d3nav.scripts.train_traj # Fine tune the video model for trajectory prediction python3 -m d3nav.scripts.train","title":"Usage"},{"location":"#model-predictive-control","text":"Checkout our Model Predictive Controller for computing steering angle and acceleration. # TODO: implement MPC to show steering # pip install model_predictive_control import numpy as np from model_predictive_control.cost.trajectory2d_steering_penalty import ( Traj2DSteeringPenalty, ) from model_predictive_control.models.bicycle import ( BicycleModel, BicycleModelParams, ) from model_predictive_control.mpc import MPC # Initialize the Bicycle Model params = BicycleModelParams( time_step=time_step, steering_ratio=13.27, wheel_base=2.83972, speed_kp=1.0, speed_ki=0.1, speed_kd=0.05, throttle_min=-1.0, throttle_max=1.0, throttle_gain=5.0, # Max throttle corresponds to 5m/s^2 ) bicycle_model = BicycleModel(params) # Define the cost function cost = Traj2DSteeringPenalty(model=bicycle_model) # Initialize MPC Controller horizon = 20 state_dim = 4 # (x, y, theta, velocity) controls_dim = 2 # (steering_angle, velocity) mpc = MPC( model=bicycle_model, cost=cost, horizon=horizon, state_dim=state_dim, controls_dim=controls_dim, ) # Define initial state (x, y, theta, velocity) start_state = [0.0, 0.0, 0.0, 1.0] # Define desired trajectory: moving in a straight line desired_state_sequence = [[i * 1.0, i * 0.5, 0.0, 1.0] for i in range(horizon)] # Initial control sequence: assuming zero steering and constant speed initial_control_sequence = [[0.0, 1.0] for _ in range(horizon)] # Define control bounds: steering_angle between -0.5 and 0.5 radians, bounds = [[(-np.deg2rad(400), np.deg2rad(400)), (-1.0, 1.0)] for _ in range(horizon)] # Optimize control inputs using MPC optimized_control_sequence = mpc.step( start_state_tuple=start_state, desired_state_sequence=desired_state_sequence, initial_control_sequence=initial_control_sequence, bounds=bounds, max_iters=50, )","title":"Model Predictive Control"},{"location":"#development","text":"Read the CONTRIBUTING.md file.","title":"Development"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#docker-environment","text":"To build, use: DOCKER_BUILDKIT=1 docker-compose build To run the interactive shell, use: docker-compose run dev","title":"Docker Environment"},{"location":"#future-video-prediction","text":"D\u00b3Nav takes 6 frames as input context and produces the next 6 frames. In the prompt columns, we show the last frame of the input and on the Prediction column, we have an animation of D\u00b3Nav's prediction of what it thinks will happen next.","title":"Future Video Prediction"},{"location":"#trajectory-demo","text":"We have put together a demo video of D\u00b3Nav operating on a subset of our dataset. In the video, D\u00b3Nav takes the video frames as input and produces the control signal (desired trajectory) as output which is plotted out as a red strip on the 3D and 2D views. We have a parallel system which produces 3D semantic occupancy. The 3D semantic occupancy is not produced by D\u00b3Nav and is only plotted to help visualize the trajectory in 3D with respect to other objects. The semantics are highlighted in both the 2D and 3D views (Vehicles in Blue and Pedestrians in Red). All other objects are colored by a height map on the 3D view. The video is placed at DEMO_3_control_signal_trajectory.mp4","title":"Trajectory Demo"},{"location":"#dataset","text":"We have provided a subset of our dataset in the BengaluruDrivingEmbeddings folder for review. We have ensured that there is no personally identifyable information (faces, number plates, etc.) in our dataset. Dataset Structure BengaluruDrivingEmbeddings/ \u251c\u2500\u2500 1658384924059 # Dataset ID \u2502 \u251c\u2500\u2500 embeddings # Folder of embeddings \u2502 \u251c\u2500\u2500 embeddings_features_quantized # Folder of quantized embeddings \u2502 \u251c\u2500\u2500 embeddings_index.npy # Integer indices of the embeddings \u2502 \u251c\u2500\u2500 input_video.mp4 # Raw video \u2502 \u2514\u2500\u2500 reconstructed_video.mp4 # Video Reconstructed by VQ-VAE \u251c\u2500\u2500 calibration # Camera Intrinsics \u2502 \u251c\u2500\u2500 calibrationSession.mat \u2502 \u251c\u2500\u2500 calib.txt \u2502 \u2514\u2500\u2500 calib.yaml \u2514\u2500\u2500 weights # VQ-VAE weights \u251c\u2500\u2500 decoder.onnx \u251c\u2500\u2500 decoder.onnx.dynanic_quant.onnx \u251c\u2500\u2500 decoder.pth \u251c\u2500\u2500 encoder.onnx \u251c\u2500\u2500 encoder.onnx.dynanic_quant.onnx \u251c\u2500\u2500 encoder.pth \u251c\u2500\u2500 quantizer_e_i_ts.npy \u251c\u2500\u2500 quantizer.onnx \u251c\u2500\u2500 quantizer.onnx.dynanic_quant.onnx \u2514\u2500\u2500 quantizer.pth","title":"Dataset"},{"location":"#acknowledgement","text":"We would like to thank the authors of the following repositories minGPT PySLAM","title":"Acknowledgement"},{"location":"CONTRIBUTING/","text":"How to develop on this project d3nav welcomes contributions from the community. You need PYTHON3! This instructions are for linux base systems. (Linux, MacOS, BSD, etc.) Setting up your own fork of this repo. On github interface click on Fork button. Clone your fork of this repo. git clone git@github.com:YOUR_GIT_USERNAME/d3_nav.git Enter the directory cd d3_nav Add upstream repo git remote add upstream https://github.com/AdityaNG/d3_nav Setting up your own virtual environment Run make virtualenv to create a virtual environment. then activate it with source .venv/bin/activate . Install the project in develop mode Run make install to install the project in develop mode. Run the tests to ensure everything is working Run make test to run the tests. Create a new branch to work on your contribution Run git checkout -b my_contribution Make your changes Edit the files using your preferred editor. (we recommend VIM or VSCode) Format the code Run make fmt to format the code. Run the linter Run make lint to run the linter. Test your changes Run make test to run the tests. Ensure code coverage report shows 100% coverage, add tests to your PR. Build the docs locally Run make docs to build the docs. Ensure your new changes are documented. Commit your changes This project uses conventional git commit messages . Example: fix(package): update setup.py arguments \ud83c\udf89 (emojis are fine too) Push your changes to your fork Run git push origin my_contribution Submit a pull request On github interface, click on Pull Request button. Wait CI to run and one of the developers will review your PR. Makefile utilities This project comes with a Makefile that contains a number of useful utility. \u276f make Usage: make <target> Targets: help: ## Show the help. install: ## Install the project in dev mode. fmt: ## Format code using black & isort. lint: ## Run pep8, black, mypy linters. test: lint ## Run tests and generate coverage report. watch: ## Run tests on every change. clean: ## Clean unused files. virtualenv: ## Create a virtual environment. release: ## Create a new tag for release. docs: ## Build the documentation. switch-to-poetry: ## Switch to poetry package manager. init: ## Initialize the project based on an application template. Making a new release This project uses semantic versioning and tags releases with X.Y.Z Every time a new tag is created and pushed to the remote repo, github actions will automatically create a new release on github and trigger a release on PyPI. For this to work you need to setup a secret called PIPY_API_TOKEN on the project settings>secrets, this token can be generated on pypi.org . To trigger a new release all you need to do is. If you have changes to add to the repo Make your changes following the steps described above. Commit your changes following the conventional git commit messages . Run the tests to ensure everything is working. Run make release to create a new tag and push it to the remote repo. the make release will ask you the version number to create the tag, ex: type 0.1.1 when you are asked. CAUTION : The make release will change local changelog files and commit all the unstaged changes you have.","title":"How to develop on this project"},{"location":"CONTRIBUTING/#how-to-develop-on-this-project","text":"d3nav welcomes contributions from the community. You need PYTHON3! This instructions are for linux base systems. (Linux, MacOS, BSD, etc.)","title":"How to develop on this project"},{"location":"CONTRIBUTING/#setting-up-your-own-fork-of-this-repo","text":"On github interface click on Fork button. Clone your fork of this repo. git clone git@github.com:YOUR_GIT_USERNAME/d3_nav.git Enter the directory cd d3_nav Add upstream repo git remote add upstream https://github.com/AdityaNG/d3_nav","title":"Setting up your own fork of this repo."},{"location":"CONTRIBUTING/#setting-up-your-own-virtual-environment","text":"Run make virtualenv to create a virtual environment. then activate it with source .venv/bin/activate .","title":"Setting up your own virtual environment"},{"location":"CONTRIBUTING/#install-the-project-in-develop-mode","text":"Run make install to install the project in develop mode.","title":"Install the project in develop mode"},{"location":"CONTRIBUTING/#run-the-tests-to-ensure-everything-is-working","text":"Run make test to run the tests.","title":"Run the tests to ensure everything is working"},{"location":"CONTRIBUTING/#create-a-new-branch-to-work-on-your-contribution","text":"Run git checkout -b my_contribution","title":"Create a new branch to work on your contribution"},{"location":"CONTRIBUTING/#make-your-changes","text":"Edit the files using your preferred editor. (we recommend VIM or VSCode)","title":"Make your changes"},{"location":"CONTRIBUTING/#format-the-code","text":"Run make fmt to format the code.","title":"Format the code"},{"location":"CONTRIBUTING/#run-the-linter","text":"Run make lint to run the linter.","title":"Run the linter"},{"location":"CONTRIBUTING/#test-your-changes","text":"Run make test to run the tests. Ensure code coverage report shows 100% coverage, add tests to your PR.","title":"Test your changes"},{"location":"CONTRIBUTING/#build-the-docs-locally","text":"Run make docs to build the docs. Ensure your new changes are documented.","title":"Build the docs locally"},{"location":"CONTRIBUTING/#commit-your-changes","text":"This project uses conventional git commit messages . Example: fix(package): update setup.py arguments \ud83c\udf89 (emojis are fine too)","title":"Commit your changes"},{"location":"CONTRIBUTING/#push-your-changes-to-your-fork","text":"Run git push origin my_contribution","title":"Push your changes to your fork"},{"location":"CONTRIBUTING/#submit-a-pull-request","text":"On github interface, click on Pull Request button. Wait CI to run and one of the developers will review your PR.","title":"Submit a pull request"},{"location":"CONTRIBUTING/#makefile-utilities","text":"This project comes with a Makefile that contains a number of useful utility. \u276f make Usage: make <target> Targets: help: ## Show the help. install: ## Install the project in dev mode. fmt: ## Format code using black & isort. lint: ## Run pep8, black, mypy linters. test: lint ## Run tests and generate coverage report. watch: ## Run tests on every change. clean: ## Clean unused files. virtualenv: ## Create a virtual environment. release: ## Create a new tag for release. docs: ## Build the documentation. switch-to-poetry: ## Switch to poetry package manager. init: ## Initialize the project based on an application template.","title":"Makefile utilities"},{"location":"CONTRIBUTING/#making-a-new-release","text":"This project uses semantic versioning and tags releases with X.Y.Z Every time a new tag is created and pushed to the remote repo, github actions will automatically create a new release on github and trigger a release on PyPI. For this to work you need to setup a secret called PIPY_API_TOKEN on the project settings>secrets, this token can be generated on pypi.org . To trigger a new release all you need to do is. If you have changes to add to the repo Make your changes following the steps described above. Commit your changes following the conventional git commit messages . Run the tests to ensure everything is working. Run make release to create a new tag and push it to the remote repo. the make release will ask you the version number to create the tag, ex: type 0.1.1 when you are asked. CAUTION : The make release will change local changelog files and commit all the unstaged changes you have.","title":"Making a new release"},{"location":"EXPERIMENTS/","text":"D3Nav v2 Experiemtns We run the following exeriments: Model L2 1s L2 2s L2 3s Train Loss Val Loss ResNet (Pure) 1.39 3.0 4.5 4.4 2.5 ResNet-Traj (Frozen) 1.13 2.0 2.8 1.0 1.0 ResNet-Traj-ft (Unfrozen) 1.08 2.0 3.0 1.0 1.1 D3Nav-3L 1.14 1.4 2.0 0.8 0.8 D3Nav-3L-ft 0.76 1.4 2.0 0.3 0.8 D3Nav-3L-ft-CA 0.66 1.2 1.8 0.2 0.7 D3Nav-3L-ft-CA-D0.2 1.00849 1.7547 2.5741 0.3725 0.96 Notes: - First we pretrain our TrajectoryEncoder and TrajectoryDecoder on the task of trajectory reconstruction. We use these latents as an interface for our models to predict trajectory. - We start of by training baseline models using the ResNet family - The baseline runs get to a minimum L2 (1s) of ~1.08 - Our baseline runs showed us that using the pretrained trajectory decoder helps, unfreezing the decoder helps even further - D3Nav-3L: We fine tune the world model GPT backbone for the task of driving by unfreezing the last 3 layers and taking the BOS token and feeding that into the trajectory decoder. This put a heavy load on the single token and we reached an L2 (1s) of 1.14 - D3Nav-3L-ft: Taking our learnings from the baseline experiment, we unfreeze the trajectory decoder, this improves the performance drastically as the backbone is no longer bottlenecked by its ability to interface with the frozen trajectory decoder. This brings us to L2 (1s) pf 0.76 - D3Nav-3L-ft-CA: in order to reduce the load on the single token predicting the trajectory, we use the entire last layer of the transformer and we apply ChunkedAttention . This further improves our performance to L2 (1s) of 0.66 - D3Nav-3L-ft-CA-D0.2: then we apply dropout to the inputs to help the model generalize further, but this dropout might have been too agressive. We have put this on pause for now (future work) Key observations: 1. D3Nav outperforms the ResNet baseline 2. Adding the trajectory decoder (ResNet-Traj) significantly improves performance 3. The D3Nav variants show progressive improvements 4. D3Nav-3L-ft-CA-D0.2 dropout rate was too agressive Baseline ResNet-34 ResNet: Pure ResNet regressing 6x2 trajectory ResNet-Traj: ResNet with frozen Trajectory Decoder ResNet-Traj-ft: ResNet with unfrozen Trajectory Decoder fine tuned D3Nav D3Nav-3L : Unfrozen 3 Layers, single token, frozen Trajectory Decoder D3Nav-3L-ft : Unfrozen 3 Layers, single token, unfrozen Trajectory Decoder D3Nav-3L-ft-CA : Unfrozen 3 Layers, unfrozen Trajectory Decoder, chunked attention D3Nav-3L-ft-CA-D0.2 : Unfrozen 3 Layers, unfrozen Trajectory Decoder, chunked attention, with frame level dropout of 20%","title":"D3Nav v2 Experiemtns"},{"location":"EXPERIMENTS/#d3nav-v2-experiemtns","text":"We run the following exeriments: Model L2 1s L2 2s L2 3s Train Loss Val Loss ResNet (Pure) 1.39 3.0 4.5 4.4 2.5 ResNet-Traj (Frozen) 1.13 2.0 2.8 1.0 1.0 ResNet-Traj-ft (Unfrozen) 1.08 2.0 3.0 1.0 1.1 D3Nav-3L 1.14 1.4 2.0 0.8 0.8 D3Nav-3L-ft 0.76 1.4 2.0 0.3 0.8 D3Nav-3L-ft-CA 0.66 1.2 1.8 0.2 0.7 D3Nav-3L-ft-CA-D0.2 1.00849 1.7547 2.5741 0.3725 0.96 Notes: - First we pretrain our TrajectoryEncoder and TrajectoryDecoder on the task of trajectory reconstruction. We use these latents as an interface for our models to predict trajectory. - We start of by training baseline models using the ResNet family - The baseline runs get to a minimum L2 (1s) of ~1.08 - Our baseline runs showed us that using the pretrained trajectory decoder helps, unfreezing the decoder helps even further - D3Nav-3L: We fine tune the world model GPT backbone for the task of driving by unfreezing the last 3 layers and taking the BOS token and feeding that into the trajectory decoder. This put a heavy load on the single token and we reached an L2 (1s) of 1.14 - D3Nav-3L-ft: Taking our learnings from the baseline experiment, we unfreeze the trajectory decoder, this improves the performance drastically as the backbone is no longer bottlenecked by its ability to interface with the frozen trajectory decoder. This brings us to L2 (1s) pf 0.76 - D3Nav-3L-ft-CA: in order to reduce the load on the single token predicting the trajectory, we use the entire last layer of the transformer and we apply ChunkedAttention . This further improves our performance to L2 (1s) of 0.66 - D3Nav-3L-ft-CA-D0.2: then we apply dropout to the inputs to help the model generalize further, but this dropout might have been too agressive. We have put this on pause for now (future work) Key observations: 1. D3Nav outperforms the ResNet baseline 2. Adding the trajectory decoder (ResNet-Traj) significantly improves performance 3. The D3Nav variants show progressive improvements 4. D3Nav-3L-ft-CA-D0.2 dropout rate was too agressive","title":"D3Nav v2 Experiemtns"},{"location":"EXPERIMENTS/#baseline-resnet-34","text":"ResNet: Pure ResNet regressing 6x2 trajectory ResNet-Traj: ResNet with frozen Trajectory Decoder ResNet-Traj-ft: ResNet with unfrozen Trajectory Decoder fine tuned","title":"Baseline ResNet-34"},{"location":"EXPERIMENTS/#d3nav","text":"D3Nav-3L : Unfrozen 3 Layers, single token, frozen Trajectory Decoder D3Nav-3L-ft : Unfrozen 3 Layers, single token, unfrozen Trajectory Decoder D3Nav-3L-ft-CA : Unfrozen 3 Layers, unfrozen Trajectory Decoder, chunked attention D3Nav-3L-ft-CA-D0.2 : Unfrozen 3 Layers, unfrozen Trajectory Decoder, chunked attention, with frame level dropout of 20%","title":"D3Nav"}]}